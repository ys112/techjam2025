{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "155090e0",
   "metadata": {},
   "source": [
    "# üèÜ TechJam 2025: Review Quality Assessment System\n",
    "\n",
    "## üéØ Challenge: ML for Trustworthy Location Reviews\n",
    "\n",
    "This notebook will guide you through building a system to detect policy violations in Google location reviews:\n",
    "- üö´ **Advertisements**: Reviews containing promotional content\n",
    "- üö´ **Irrelevant Content**: Reviews not related to the location\n",
    "- üö´ **Fake Rants**: Complaints from users who never visited\n",
    "\n",
    "**Today's Goal (Day 1)**: Set up environment, explore data, and build basic understanding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807423ad",
   "metadata": {},
   "source": [
    "## üìö Step 1: Import Required Libraries\n",
    "\n",
    "Let's start by importing all the libraries we'll need for data processing, ML models, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e838ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this if packages are not installed)\n",
    "# Uncomment the lines below if you need to install packages\n",
    "\n",
    "# !pip install pandas numpy matplotlib seaborn\n",
    "# !pip install transformers torch\n",
    "# !pip install huggingface_hub\n",
    "# !pip install scikit-learn\n",
    "# !pip install streamlit --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d305d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML and NLP libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "\n",
    "# Hugging Face transformers\n",
    "try:\n",
    "    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "    print(\"‚úÖ Transformers library loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Transformers not installed. Please run: pip install transformers torch\")\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6062528",
   "metadata": {},
   "source": [
    "## üìä Step 2: Data Loading and Initial Exploration\n",
    "\n",
    "First, let's load the Google Reviews dataset and understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_data():\n",
    "    \"\"\"\n",
    "    Create sample data for testing if you don't have the dataset yet.\n",
    "    Replace this with actual data loading when you get the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample reviews with different violation types\n",
    "    sample_reviews = [\n",
    "        # Normal reviews\n",
    "        {\"review_text\": \"Great food and excellent service. The pasta was delicious and the staff was very friendly. Highly recommend!\", \"rating\": 5, \"business_name\": \"Mario's Restaurant\"},\n",
    "        {\"review_text\": \"Average experience. Food was okay but service was slow. Not bad but not great either.\", \"rating\": 3, \"business_name\": \"Central Cafe\"},\n",
    "        {\"review_text\": \"Terrible experience. Food was cold and the waiter was rude. Will not return.\", \"rating\": 1, \"business_name\": \"Downtown Diner\"},\n",
    "        \n",
    "        # Advertisement examples\n",
    "        {\"review_text\": \"Amazing pizza! Visit our website www.pizzadeals.com for 50% off coupons and special offers!\", \"rating\": 5, \"business_name\": \"Tony's Pizza\"},\n",
    "        {\"review_text\": \"Great burgers! Call us at 555-BURGER for catering services and party packages!\", \"rating\": 5, \"business_name\": \"Burger Palace\"},\n",
    "        {\"review_text\": \"Delicious food! Check out our new location on Main Street. Grand opening specials available!\", \"rating\": 5, \"business_name\": \"Fresh Bites\"},\n",
    "        \n",
    "        # Irrelevant content examples\n",
    "        {\"review_text\": \"I love my new smartphone camera! Anyway, this restaurant has okay food I guess.\", \"rating\": 3, \"business_name\": \"City Grill\"},\n",
    "        {\"review_text\": \"Traffic was terrible today because of construction. Politics are crazy these days. Oh, the coffee was fine.\", \"rating\": 3, \"business_name\": \"Corner Coffee\"},\n",
    "        {\"review_text\": \"My car broke down on the way here, what a terrible day. The weather is also awful. Food was decent though.\", \"rating\": 2, \"business_name\": \"Highway Diner\"},\n",
    "        \n",
    "        # Fake rant examples\n",
    "        {\"review_text\": \"Never been here but I heard from my neighbor that it's absolutely terrible. Probably overpriced too.\", \"rating\": 1, \"business_name\": \"Elite Restaurant\"},\n",
    "        {\"review_text\": \"I hate these fancy restaurants, they're all scams. Never visited but I'm sure it's pretentious.\", \"rating\": 1, \"business_name\": \"Fine Dining Co\"},\n",
    "        {\"review_text\": \"Looks dirty from the outside, probably awful inside too. Won't waste my time going there.\", \"rating\": 1, \"business_name\": \"Street Food Truck\"}\n",
    "    ]\n",
    "    \n",
    "    return pd.DataFrame(sample_reviews)\n",
    "\n",
    "# Load data\n",
    "# TODO: Replace this with actual dataset loading\n",
    "# df = pd.read_csv('path_to_google_reviews_dataset.csv')\n",
    "\n",
    "# For now, use sample data\n",
    "df = load_sample_data()\n",
    "\n",
    "print(f\"üìä Dataset loaded with {len(df)} reviews\")\n",
    "print(f\"üìã Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nüìù First 3 reviews:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da4d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data exploration\n",
    "def explore_data(df):\n",
    "    \"\"\"\n",
    "    Perform basic exploration of the review dataset\n",
    "    \"\"\"\n",
    "    print(\"üîç BASIC DATA EXPLORATION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Dataset info\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Text statistics\n",
    "    df['review_length'] = df['review_text'].str.len()\n",
    "    df['word_count'] = df['review_text'].str.split().str.len()\n",
    "    \n",
    "    print(f\"\\nüìè Review Length Statistics:\")\n",
    "    print(f\"  Average length: {df['review_length'].mean():.1f} characters\")\n",
    "    print(f\"  Average words: {df['word_count'].mean():.1f} words\")\n",
    "    print(f\"  Shortest review: {df['review_length'].min()} characters\")\n",
    "    print(f\"  Longest review: {df['review_length'].max()} characters\")\n",
    "    \n",
    "    # Rating distribution\n",
    "    print(f\"\\n‚≠ê Rating Distribution:\")\n",
    "    print(df['rating'].value_counts().sort_index())\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = explore_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbb2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Review length distribution\n",
    "axes[0, 0].hist(df['review_length'], bins=20, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Distribution of Review Length (Characters)')\n",
    "axes[0, 0].set_xlabel('Characters')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Word count distribution\n",
    "axes[0, 1].hist(df['word_count'], bins=20, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Distribution of Word Count')\n",
    "axes[0, 1].set_xlabel('Words')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Rating distribution\n",
    "rating_counts = df['rating'].value_counts().sort_index()\n",
    "axes[1, 0].bar(rating_counts.index, rating_counts.values, alpha=0.7, color='orange')\n",
    "axes[1, 0].set_title('Distribution of Ratings')\n",
    "axes[1, 0].set_xlabel('Rating')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "# Review length vs rating scatter\n",
    "axes[1, 1].scatter(df['rating'], df['review_length'], alpha=0.6, color='purple')\n",
    "axes[1, 1].set_title('Review Length vs Rating')\n",
    "axes[1, 1].set_xlabel('Rating')\n",
    "axes[1, 1].set_ylabel('Review Length (Characters)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Data visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60371037",
   "metadata": {},
   "source": [
    "## üîß Step 3: Feature Engineering\n",
    "\n",
    "Let's extract useful features that can help identify policy violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d179ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    \"\"\"\n",
    "    Extract features that might indicate policy violations\n",
    "    \"\"\"\n",
    "    print(\"üîß EXTRACTING FEATURES FOR VIOLATION DETECTION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic text features\n",
    "    df['review_length'] = df['review_text'].str.len()\n",
    "    df['word_count'] = df['review_text'].str.split().str.len()\n",
    "    df['exclamation_count'] = df['review_text'].str.count('!')\n",
    "    df['question_count'] = df['review_text'].str.count('\\?')\n",
    "    \n",
    "    # Capitalization features (potential indicators of spam/rants)\n",
    "    df['caps_ratio'] = df['review_text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
    "    df['excessive_caps'] = df['caps_ratio'] > 0.3\n",
    "    \n",
    "    # Advertisement indicators\n",
    "    df['has_url'] = df['review_text'].str.contains(r'http[s]?://|www\\.', regex=True, na=False)\n",
    "    df['has_phone'] = df['review_text'].str.contains(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b|call|phone', regex=True, na=False, case=False)\n",
    "    df['has_promo_words'] = df['review_text'].str.contains(r'discount|deal|promo|sale|coupon|special offer|visit|website', regex=True, na=False, case=False)\n",
    "    \n",
    "    # Irrelevant content indicators\n",
    "    df['mentions_unrelated'] = df['review_text'].str.contains(r'my phone|my car|politics|weather|traffic|news|government', regex=True, na=False, case=False)\n",
    "    \n",
    "    # Fake rant indicators\n",
    "    df['never_visited'] = df['review_text'].str.contains(r'never been|never visited|heard it|looks like|probably|i hate these|all these places', regex=True, na=False, case=False)\n",
    "    \n",
    "    # Length-based features\n",
    "    df['very_short'] = df['word_count'] < 5\n",
    "    df['very_long'] = df['word_count'] > 200\n",
    "    \n",
    "    print(f\"‚úÖ Extracted {len([col for col in df.columns if col not in ['review_text', 'rating', 'business_name']])} features\")\n",
    "    \n",
    "    # Show feature summary\n",
    "    feature_cols = ['has_url', 'has_phone', 'has_promo_words', 'mentions_unrelated', 'never_visited', 'excessive_caps']\n",
    "    print(\"\\nüìä Feature Summary:\")\n",
    "    for col in feature_cols:\n",
    "        count = df[col].sum()\n",
    "        print(f\"  {col}: {count} reviews ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = extract_features(df)\n",
    "print(\"\\nüìã Sample of extracted features:\")\n",
    "df[['review_text', 'has_url', 'has_promo_words', 'mentions_unrelated', 'never_visited']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1a97eb",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Step 4: Manual Labeling (Create Ground Truth)\n",
    "\n",
    "Let's manually label our sample data to create ground truth for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd89c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_manual_labels(df):\n",
    "    \"\"\"\n",
    "    Create manual labels for the sample data\n",
    "    In a real scenario, you would label a larger subset manually\n",
    "    \"\"\"\n",
    "    print(\"üè∑Ô∏è CREATING MANUAL LABELS FOR GROUND TRUTH\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Manual labels based on our sample data\n",
    "    # 0 = No violation, 1 = Violation\n",
    "    \n",
    "    # Advertisement labels (reviews 3, 4, 5 in our sample)\n",
    "    ad_labels = [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    # Irrelevant content labels (reviews 6, 7, 8 in our sample)\n",
    "    irrelevant_labels = [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0]\n",
    "    \n",
    "    # Fake rant labels (reviews 9, 10, 11 in our sample)\n",
    "    fake_rant_labels = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
    "    \n",
    "    df['is_advertisement'] = ad_labels\n",
    "    df['is_irrelevant'] = irrelevant_labels\n",
    "    df['is_fake_rant'] = fake_rant_labels\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"üìä Label Summary:\")\n",
    "    print(f\"  Advertisements: {df['is_advertisement'].sum()}/{len(df)} ({df['is_advertisement'].sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Irrelevant: {df['is_irrelevant'].sum()}/{len(df)} ({df['is_irrelevant'].sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Fake Rants: {df['is_fake_rant'].sum()}/{len(df)} ({df['is_fake_rant'].sum()/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = create_manual_labels(df)\n",
    "print(\"\\n‚úÖ Manual labels created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b14dcc",
   "metadata": {},
   "source": [
    "## ü§ñ Step 5: LLM-Based Classifier with Real Language Models\n",
    "\n",
    "Now let's implement a real language model-based classifier using Hugging Face transformers with prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91969d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier:\n",
    "    \"\"\"\n",
    "    LLM-based classifier for policy violation detection using Hugging Face transformers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "        \"\"\"Initialize the review classifier with specified model\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.classifier = None\n",
    "        self.text_generator = None\n",
    "        self._setup_model()\n",
    "    \n",
    "    def _setup_model(self):\n",
    "        \"\"\"Setup the Hugging Face model\"\"\"\n",
    "        # Always setup fallback keywords first\n",
    "        self._setup_fallback()\n",
    "        \n",
    "        try:\n",
    "            # Try to load a text generation model for prompt-based classification\n",
    "            print(f\"ü§ñ Loading language model: {self.model_name}\")\n",
    "            \n",
    "            # For lighter models, use text-classification pipeline directly\n",
    "            if 'distilbert' in self.model_name.lower() or 'bert' in self.model_name.lower():\n",
    "                self.classifier = pipeline(\n",
    "                    \"text-classification\",\n",
    "                    model=self.model_name,\n",
    "                    device=-1  # Use CPU for compatibility\n",
    "                )\n",
    "                print(\"‚úÖ Loaded classification model successfully\")\n",
    "            else:\n",
    "                # For larger LLMs, use text generation\n",
    "                self.text_generator = pipeline(\n",
    "                    \"text-generation\",\n",
    "                    model=self.model_name,\n",
    "                    device=-1,\n",
    "                    max_length=512,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                print(\"‚úÖ Loaded text generation model successfully\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {self.model_name}: {e}\")\n",
    "            print(\"üîÑ Falling back to rule-based classification\")\n",
    "            self._setup_fallback()\n",
    "    \n",
    "    def _setup_fallback(self):\n",
    "        \"\"\"Setup fallback rule-based classification\"\"\"\n",
    "        self.ad_keywords = [\n",
    "            'visit', 'website', 'www', 'http', 'call', 'phone', 'discount',\n",
    "            'deal', 'promo', 'sale', 'coupon', 'special offer'\n",
    "        ]\n",
    "        \n",
    "        self.irrelevant_keywords = [\n",
    "            'my phone', 'my car', 'politics', 'weather', 'traffic',\n",
    "            'my day', 'my life', 'news', 'government', 'president'\n",
    "        ]\n",
    "        \n",
    "        self.fake_rant_keywords = [\n",
    "            'never been', 'heard it', 'looks like', 'probably',\n",
    "            'i hate these', 'all these places', 'never visited'\n",
    "        ]\n",
    "    \n",
    "    def create_prompts(self) -> dict:\n",
    "        \"\"\"Create prompts for each policy violation type\"\"\"\n",
    "        return {\n",
    "            'advertisement': '''\n",
    "Task: Determine if this review contains advertisements or promotional content.\n",
    "\n",
    "Examples of ADVERTISEMENTS:\n",
    "- \"Great food! Visit www.discount-deals.com for coupons!\"\n",
    "- \"Call 555-1234 for catering services!\"\n",
    "- \"Check out our new location on Main Street!\"\n",
    "\n",
    "Examples of NOT ADVERTISEMENTS:\n",
    "- \"The food was delicious and service was great\"\n",
    "- \"I loved the atmosphere, will definitely come back\"\n",
    "- \"Terrible experience, would not recommend\"\n",
    "\n",
    "Review to analyze: \"{review_text}\"\n",
    "\n",
    "Is this review an ADVERTISEMENT? Answer only YES or NO:''',\n",
    "\n",
    "            'irrelevant': '''\n",
    "Task: Determine if this review is about the business location being reviewed.\n",
    "\n",
    "Examples of IRRELEVANT reviews:\n",
    "- \"I love my new phone, but this place is noisy\" (about phone, not restaurant)\n",
    "- \"My car broke down on the way here, terrible day\" (about car, not business)\n",
    "- \"Politics these days are crazy, anyway the food was ok\" (mostly about politics)\n",
    "\n",
    "Examples of RELEVANT reviews:\n",
    "- \"The pizza was amazing, great service too\"\n",
    "- \"Parking was difficult but the experience was worth it\"\n",
    "- \"Staff was rude and food was cold\"\n",
    "\n",
    "Review to analyze: \"{review_text}\"\n",
    "\n",
    "Is this review IRRELEVANT to the business? Answer only YES or NO:''',\n",
    "\n",
    "            'fake_rant': '''\n",
    "Task: Determine if this is a rant from someone who likely never visited the place.\n",
    "\n",
    "Examples of FAKE RANTS:\n",
    "- \"Never been here but heard it's terrible from my friend\"\n",
    "- \"I hate this type of business, they're all scams\"\n",
    "- \"Looks dirty from the outside, probably awful inside\"\n",
    "\n",
    "Examples of GENUINE reviews (even if negative):\n",
    "- \"I visited yesterday and the service was terrible\"\n",
    "- \"Went there for lunch, food was cold and overpriced\"\n",
    "- \"Been there multiple times, quality has declined\"\n",
    "\n",
    "Review to analyze: \"{review_text}\"\n",
    "\n",
    "Is this a FAKE RANT from someone who likely never visited? Answer only YES or NO:'''\n",
    "        }\n",
    "    \n",
    "    def _classify_with_llm(self, review_text: str, violation_type: str) -> bool:\n",
    "        \"\"\"Use LLM to classify for a specific violation type\"\"\"\n",
    "        if self.text_generator is None:\n",
    "            return self._classify_fallback(review_text, violation_type)\n",
    "        \n",
    "        try:\n",
    "            prompts = self.create_prompts()\n",
    "            prompt = prompts[violation_type].format(review_text=review_text)\n",
    "            \n",
    "            response = self.text_generator(\n",
    "                prompt,\n",
    "                max_new_tokens=10,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.text_generator.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            generated_text = response[0]['generated_text'][len(prompt):].strip().upper()\n",
    "            return 'YES' in generated_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è LLM classification failed for {violation_type}: {e}\")\n",
    "            return self._classify_fallback(review_text, violation_type)\n",
    "    \n",
    "    def _classify_with_bert(self, review_text: str, violation_type: str) -> bool:\n",
    "        \"\"\"Use BERT-style model for classification with heuristics\"\"\"\n",
    "        if self.classifier is None:\n",
    "            return self._classify_fallback(review_text, violation_type)\n",
    "        \n",
    "        try:\n",
    "            # For BERT models, we'll use sentiment analysis as a proxy and combine with heuristics\n",
    "            result = self.classifier(review_text)\n",
    "            sentiment_score = result[0]['score'] if result[0]['label'] == 'POSITIVE' else 1 - result[0]['score']\n",
    "            \n",
    "            # Combine sentiment with rule-based heuristics for better accuracy\n",
    "            rule_based = self._classify_fallback(review_text, violation_type)\n",
    "            \n",
    "            # Advertisement: typically positive sentiment + promotional keywords\n",
    "            if violation_type == 'advertisement':\n",
    "                return rule_based or (sentiment_score > 0.8 and any(kw in review_text.lower() \n",
    "                                                                   for kw in ['www', 'http', 'call', 'visit']))\n",
    "            \n",
    "            # Irrelevant: often contains off-topic keywords regardless of sentiment\n",
    "            elif violation_type == 'irrelevant':\n",
    "                return rule_based\n",
    "            \n",
    "            # Fake rant: typically very negative + fake indicators\n",
    "            elif violation_type == 'fake_rant':\n",
    "                return rule_based or (sentiment_score < 0.3 and any(kw in review_text.lower() \n",
    "                                                                   for kw in ['never been', 'heard', 'probably']))\n",
    "            \n",
    "            return rule_based\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è BERT classification failed for {violation_type}: {e}\")\n",
    "            return self._classify_fallback(review_text, violation_type)\n",
    "    \n",
    "    def _classify_fallback(self, review_text: str, violation_type: str) -> bool:\n",
    "        \"\"\"Fallback rule-based classification\"\"\"\n",
    "        text_lower = review_text.lower()\n",
    "        \n",
    "        if violation_type == 'advertisement':\n",
    "            return any(keyword in text_lower for keyword in self.ad_keywords)\n",
    "        elif violation_type == 'irrelevant':\n",
    "            return any(keyword in text_lower for keyword in self.irrelevant_keywords)\n",
    "        elif violation_type == 'fake_rant':\n",
    "            return any(keyword in text_lower for keyword in self.fake_rant_keywords)\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def classify_review(self, text: str) -> dict:\n",
    "        \"\"\"Classify a single review for all violation types\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for violation_type in ['advertisement', 'irrelevant', 'fake_rant']:\n",
    "            if self.text_generator:\n",
    "                results[violation_type] = self._classify_with_llm(text, violation_type)\n",
    "            elif self.classifier:\n",
    "                results[violation_type] = self._classify_with_bert(text, violation_type)\n",
    "            else:\n",
    "                results[violation_type] = self._classify_fallback(text, violation_type)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def classify_batch(self, texts: list) -> list:\n",
    "        \"\"\"Classify multiple reviews\"\"\"\n",
    "        results = []\n",
    "        total = len(texts)\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            if i % 50 == 0:\n",
    "                print(f\"üìä Processing review {i+1}/{total}\")\n",
    "            results.append(self.classify_review(text))\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test the LLM-based classifier\n",
    "print(\"ü§ñ TESTING LLM-BASED CLASSIFIER\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "classifier = ReviewClassifier()\n",
    "\n",
    "# Get predictions\n",
    "reviews = df['review_text'].tolist()\n",
    "predictions = classifier.classify_batch(reviews)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df['pred_advertisement'] = [pred['advertisement'] for pred in predictions]\n",
    "df['pred_irrelevant'] = [pred['irrelevant'] for pred in predictions]\n",
    "df['pred_fake_rant'] = [pred['fake_rant'] for pred in predictions]\n",
    "\n",
    "print(\"‚úÖ LLM-based classification complete!\")\n",
    "print(f\"\\nüìä Prediction Summary:\")\n",
    "print(f\"  Predicted Advertisements: {df['pred_advertisement'].sum()}\")\n",
    "print(f\"  Predicted Irrelevant: {df['pred_irrelevant'].sum()}\")\n",
    "print(f\"  Predicted Fake Rants: {df['pred_fake_rant'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d05ff",
   "metadata": {},
   "source": [
    "## üìä Step 6: Evaluation and Metrics\n",
    "\n",
    "Let's evaluate our LLM-based classifier performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c07cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(df, violation_type):\n",
    "    \"\"\"\n",
    "    Evaluate classifier performance for a specific violation type\n",
    "    \"\"\"\n",
    "    true_col = f'is_{violation_type}'\n",
    "    pred_col = f'pred_{violation_type}'\n",
    "    \n",
    "    y_true = df[true_col].tolist()\n",
    "    y_pred = df[pred_col].tolist()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    accuracy = sum(t == p for t, p in zip(y_true, y_pred)) / len(y_true)\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(df, violation_type):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix for a violation type\n",
    "    \"\"\"\n",
    "    true_col = f'is_{violation_type}'\n",
    "    pred_col = f'pred_{violation_type}'\n",
    "    \n",
    "    y_true = df[true_col].tolist()\n",
    "    y_pred = df[pred_col].tolist()\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=['No Violation', 'Violation'],\n",
    "               yticklabels=['No Violation', 'Violation'])\n",
    "    plt.title(f'Confusion Matrix - {violation_type.title()} Detection')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate all violation types\n",
    "print(\"üìä LLM-BASED CLASSIFIER EVALUATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "violation_types = ['advertisement', 'irrelevant', 'fake_rant']\n",
    "results = {}\n",
    "\n",
    "for vtype in violation_types:\n",
    "    metrics = evaluate_classifier(df, vtype)\n",
    "    results[vtype] = metrics\n",
    "    \n",
    "    print(f\"\\nüéØ {vtype.upper()} DETECTION:\")\n",
    "    print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "    print(f\"  Recall:    {metrics['recall']:.3f}\")\n",
    "    print(f\"  F1-Score:  {metrics['f1_score']:.3f}\")\n",
    "    print(f\"  Accuracy:  {metrics['accuracy']:.3f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(df, vtype)\n",
    "\n",
    "# Overall average F1-score\n",
    "avg_f1 = np.mean([results[vtype]['f1_score'] for vtype in violation_types])\n",
    "print(f\"\\nüèÜ OVERALL AVERAGE F1-SCORE: {avg_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebcaa4a",
   "metadata": {},
   "source": [
    "## üîÆ Step 7: Advanced LLM Model Options and Comparison\n",
    "\n",
    "Let's explore different language models and compare their performance on our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc771731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_llm_models():\n",
    "    \"\"\"\n",
    "    Compare different language models for classification performance\n",
    "    \"\"\"\n",
    "    print(\"üîÆ COMPARING DIFFERENT LLM MODELS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # List of models to try (ordered by computational requirements)\n",
    "    models_to_test = [\n",
    "        \"distilbert-base-uncased-finetuned-sst-2-english\",  # Lightweight\n",
    "        \"microsoft/DialoGPT-medium\",  # Medium LLM\n",
    "        \"google/flan-t5-small\",  # Text-to-text model\n",
    "    ]\n",
    "    \n",
    "    model_results = {}\n",
    "    \n",
    "    # Test each model on a sample review\n",
    "    test_reviews = [\n",
    "        \"Great food! Visit www.discount-deals.com for coupons!\",  # Advertisement\n",
    "        \"I love my new phone, but this place is noisy\",  # Irrelevant\n",
    "        \"Never been here but heard it's terrible from my friend\"  # Fake rant\n",
    "    ]\n",
    "    \n",
    "    for model_name in models_to_test:\n",
    "        print(f\"\\nüß™ Testing {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Create classifier with this model\n",
    "            test_classifier = ReviewClassifier(model_name=model_name)\n",
    "            \n",
    "            # Test on sample reviews\n",
    "            results = []\n",
    "            for review in test_reviews:\n",
    "                prediction = test_classifier.classify_review(review)\n",
    "                results.append(prediction)\n",
    "            \n",
    "            model_results[model_name] = {\n",
    "                'status': 'success',\n",
    "                'results': results\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ {model_name} loaded and tested successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {model_name} failed: {e}\")\n",
    "            model_results[model_name] = {\n",
    "                'status': 'failed',\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    # Display comparison results\n",
    "    print(\"\\nüìä MODEL COMPARISON RESULTS:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    for model_name, result in model_results.items():\n",
    "        status = \"‚úÖ\" if result['status'] == 'success' else \"‚ùå\"\n",
    "        print(f\"{status} {model_name}: {result['status']}\")\n",
    "        \n",
    "        if result['status'] == 'success':\n",
    "            print(\"   Sample classifications:\")\n",
    "            for i, (review, pred) in enumerate(zip(test_reviews, result['results'])):\n",
    "                violations = [k for k, v in pred.items() if v]\n",
    "                violation_str = \", \".join(violations) if violations else \"No violations\"\n",
    "                print(f\"     Review {i+1}: {violation_str}\")\n",
    "    \n",
    "    return model_results\n",
    "\n",
    "# Run model comparison\n",
    "model_comparison = compare_llm_models()\n",
    "\n",
    "# Suggest best practices\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "print(\"=\" * 20)\n",
    "print(\"‚Ä¢ For production: Use DistilBERT for speed, T5 for accuracy\")\n",
    "print(\"‚Ä¢ For experimentation: Try larger models like GPT-2 or Llama\")\n",
    "print(\"‚Ä¢ Consider ensemble methods: Combine multiple model predictions\")\n",
    "print(\"‚Ä¢ Fine-tuning: Train models on domain-specific review data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bac5c1",
   "metadata": {},
   "source": [
    "## üíæ Step 8: Save Progress and Plan Next Steps\n",
    "\n",
    "Let's save our work and prepare for tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data for next session\n",
    "df.to_csv('processed_reviews_day1.csv', index=False)\n",
    "print(\"üíæ Data saved to 'processed_reviews_day1.csv'\")\n",
    "\n",
    "# Save results summary\n",
    "summary = {\n",
    "    'day': 1,\n",
    "    'date': '2025-08-25',\n",
    "    'dataset_size': len(df),\n",
    "    'features_extracted': len([col for col in df.columns if col.startswith(('has_', 'is_', 'pred_'))]),\n",
    "    'baseline_results': results,\n",
    "    'avg_f1_score': avg_f1,\n",
    "    'next_steps': [\n",
    "        'Fine-tune models on domain data',\n",
    "        'Implement ensemble approach',\n",
    "        'Test on larger dataset',\n",
    "        'Create ensemble approach'\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('day1_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"üìÑ Summary saved to 'day1_summary.json'\")\n",
    "\n",
    "print(\"\\nüéâ DAY 1 COMPLETE!\")\n",
    "print(\"=\" * 20)\n",
    "print(\"‚úÖ Environment set up\")\n",
    "print(\"‚úÖ Data loaded and explored\")\n",
    "print(\"‚úÖ Features extracted\")\n",
    "print(\"‚úÖ LLM-based classifier implemented\")\n",
    "print(\"‚úÖ Real language models integrated\")\n",
    "print(\"‚úÖ Evaluation metrics calculated\")\n",
    "print(f\"‚úÖ LLM F1-score: {avg_f1:.3f}\")\n",
    "\n",
    "print(\"\\nüìÖ TOMORROW'S PLAN (Day 2):\")\n",
    "print(\"üéØ Fine-tune models on domain data\")\n",
    "print(\"üéØ Implement ensemble approach\")\n",
    "print(\"üéØ Test and improve accuracy\")\n",
    "print(\"üéØ Prepare for demo creation\")\n",
    "\n",
    "print(\"\\nüèÜ Great job! You're on track to win this hackathon! üèÜ\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
