{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d65756e",
   "metadata": {},
   "source": [
    "# üè∑Ô∏è Multi-Label Text Classification Pipeline\n",
    "\n",
    "This notebook implements a hybrid classification approach:\n",
    "\n",
    "1. **Regex-based rules** for obvious patterns (fast)\n",
    "2. **Zero-shot classification** for ambiguous cases (accurate)\n",
    "3. **Combined labeling** for final multi-label results\n",
    "\n",
    "**Target Labels:** Advertisement, Irrelevant, Fake_Rant\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14bd7b5",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7c300d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Setting up dependencies...\n",
      "‚úÖ transformers>=4.21.0 ready\n",
      "‚úÖ transformers>=4.21.0 ready\n",
      "‚úÖ torch ready\n",
      "‚úÖ torch ready\n",
      "‚úÖ pandas ready\n",
      "‚úÖ pandas ready\n",
      "‚úÖ numpy ready\n",
      "‚úÖ numpy ready\n",
      "‚úÖ tqdm ready\n",
      "\n",
      "üöÄ All dependencies ready for lightning-fast classification!\n",
      "‚úÖ tqdm ready\n",
      "\n",
      "üöÄ All dependencies ready for lightning-fast classification!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for optimal performance\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", package],\n",
    "            stdout=subprocess.DEVNULL,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "        )\n",
    "        print(f\"‚úÖ {package} ready\")\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è  {package} installation issue (might already be installed)\")\n",
    "\n",
    "\n",
    "print(\"üì¶ Setting up dependencies...\")\n",
    "packages = [\"transformers>=4.21.0\", \"torch\", \"pandas\", \"numpy\", \"tqdm\"]\n",
    "\n",
    "for pkg in packages:\n",
    "    install_package(pkg)\n",
    "\n",
    "print(\"\\nüöÄ All dependencies ready for lightning-fast classification!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "993217bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Environment configured:\n",
      "   Device: CPU\n",
      "   PyTorch: 2.8.0\n",
      "   Pandas: 2.3.1\n",
      "   NumPy: 1.26.4\n",
      "\n",
      "‚ö° Ready for ULTRA-FAST high-performance classification!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries with performance optimizations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Set\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Hugging Face imports with caching optimizations\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Configure for optimal performance\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tqdm.pandas()\n",
    "\n",
    "# Set device for faster inference\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "device_name = \"GPU (CUDA)\" if device == 0 else \"CPU\"\n",
    "\n",
    "print(f\"üîß Environment configured:\")\n",
    "print(f\"   Device: {device_name}\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   Pandas: {pd.__version__}\")\n",
    "print(f\"   NumPy: {np.__version__}\")\n",
    "print(f\"\\n‚ö° Ready for ULTRA-FAST high-performance classification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ef397",
   "metadata": {},
   "source": [
    "## üìä Data Loading and Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6564df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading data from: ../data/cleaned_google_reviews.csv\n",
      "‚úÖ Dataset loaded successfully!\n",
      "   üìä Shape: (673065, 17)\n",
      "   üìã Columns: ['user_id', 'user_name', 'review_time', 'rating', 'review_text', 'pics', 'resp', 'gmap_id', 'has_resp', 'resp_text', 'resp_time', 'biz_name', 'description', 'category', 'avg_rating', 'num_of_reviews', 'price_level']\n",
      "\n",
      "üìù Text Data Quality:\n",
      "   Total rows: 673,065\n",
      "   Valid text rows: 347,087 (51.6%)\n",
      "   Empty/missing: 325,978\n",
      "\n",
      "üìã Sample Data:\n",
      "   Row 0: 'Great place to care for our children.'\n",
      "   Row 1: 'Th sw y are so nice'\n",
      "   Row 2: 'Went with my daughter'\n",
      "‚úÖ Dataset loaded successfully!\n",
      "   üìä Shape: (673065, 17)\n",
      "   üìã Columns: ['user_id', 'user_name', 'review_time', 'rating', 'review_text', 'pics', 'resp', 'gmap_id', 'has_resp', 'resp_text', 'resp_time', 'biz_name', 'description', 'category', 'avg_rating', 'num_of_reviews', 'price_level']\n",
      "\n",
      "üìù Text Data Quality:\n",
      "   Total rows: 673,065\n",
      "   Valid text rows: 347,087 (51.6%)\n",
      "   Empty/missing: 325,978\n",
      "\n",
      "üìã Sample Data:\n",
      "   Row 0: 'Great place to care for our children.'\n",
      "   Row 1: 'Th sw y are so nice'\n",
      "   Row 2: 'Went with my daughter'\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare the dataset\n",
    "DATA_PATH = \"../data/cleaned_google_reviews.csv\"  # Update path as needed\n",
    "\n",
    "print(f\"üìÅ Loading data from: {DATA_PATH}\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"   üìä Shape: {df.shape}\")\n",
    "    print(f\"   üìã Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # Ensure text column exists and handle missing values\n",
    "    text_col = \"text\" if \"text\" in df.columns else \"review_text\"\n",
    "    if text_col not in df.columns:\n",
    "        raise ValueError(\n",
    "            f\"No text column found. Available columns: {df.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    # Clean and prepare text data\n",
    "    df[text_col] = df[text_col].fillna(\"\")  # Handle NaN values\n",
    "    df[text_col] = df[text_col].astype(str)  # Ensure string type\n",
    "\n",
    "    # Filter out empty texts for processing\n",
    "    valid_text_mask = (df[text_col].str.len() > 0) & (df[text_col] != \"nan\")\n",
    "    total_rows = len(df)\n",
    "    valid_rows = valid_text_mask.sum()\n",
    "\n",
    "    print(f\"\\nüìù Text Data Quality:\")\n",
    "    print(f\"   Total rows: {total_rows:,}\")\n",
    "    print(f\"   Valid text rows: {valid_rows:,} ({valid_rows/total_rows*100:.1f}%)\")\n",
    "    print(f\"   Empty/missing: {total_rows - valid_rows:,}\")\n",
    "\n",
    "    # Display sample data\n",
    "    print(f\"\\nüìã Sample Data:\")\n",
    "    sample_df = df[valid_text_mask].head(3)\n",
    "    for i, row in sample_df.iterrows():\n",
    "        text_preview = (\n",
    "            row[text_col][:100] + \"...\" if len(row[text_col]) > 100 else row[text_col]\n",
    "        )\n",
    "        print(f\"   Row {i}: '{text_preview}'\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File not found: {DATA_PATH}\")\n",
    "    print(\"Creating sample dataset for demonstration...\")\n",
    "\n",
    "    # Create sample data for demonstration\n",
    "    sample_data = {\n",
    "        \"user_id\": [\"user1\", \"user2\", \"user3\", \"user4\", \"user5\"],\n",
    "        \"text\": [\n",
    "            \"Buy now! 50% discount on all items! Call 555-1234 today!\",\n",
    "            \"hello\",\n",
    "            \"Never visited this place but heard bad things about it from friends\",\n",
    "            \"Great food and excellent service. Highly recommend this restaurant.\",\n",
    "            \"Thanks for the info\",\n",
    "        ],\n",
    "        \"rating\": [1, 3, 1, 5, 4],\n",
    "        \"category\": [\"Restaurant\", \"Store\", \"Restaurant\", \"Restaurant\", \"Store\"],\n",
    "    }\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    text_col = \"text\"\n",
    "    valid_text_mask = df[text_col].str.len() > 0\n",
    "    print(f\"‚úÖ Sample dataset created with {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf404a",
   "metadata": {},
   "source": [
    "## üîç Regex-Based Rule Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Regex Engine Initialized:\n",
      "   Advertisement: 5 patterns\n",
      "   Irrelevant: 4 patterns\n",
      "   Fake_Rant: 4 patterns\n",
      "   Total: 13 compiled regex patterns\n"
     ]
    }
   ],
   "source": [
    "class FastRuleClassifier:\n",
    "    \"\"\"Lightning-fast regex-based classification for obvious patterns\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Pre-compiled regex patterns for maximum performance\n",
    "        self.patterns = {\n",
    "            \"Advertisement\": [\n",
    "                re.compile(\n",
    "                    r\"\\b(buy now|discount|sale|offer|promo|deal|coupon|special offer)\\b\",\n",
    "                    re.IGNORECASE,\n",
    "                ),\n",
    "                re.compile(r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\"),  # Phone numbers\n",
    "                re.compile(\n",
    "                    r\"\\b(call|contact|visit|website|www\\.|http)\\b\", re.IGNORECASE\n",
    "                ),\n",
    "                re.compile(\n",
    "                    r\"\\b(free|limited time|act now|order today)\\b\", re.IGNORECASE\n",
    "                ),\n",
    "                re.compile(\n",
    "                    r\"\\$\\d+|\\b\\d+%\\s*off\\b\", re.IGNORECASE\n",
    "                ),  # Prices and discounts\n",
    "            ],\n",
    "            \"Irrelevant\": [\n",
    "                re.compile(\n",
    "                    r\"^\\s*(hello|hi|thanks|thank you|ok|okay)\\s*$\", re.IGNORECASE\n",
    "                ),\n",
    "                re.compile(r\"^\\s*\\w{1,4}\\s*$\"),  # Very short single words\n",
    "                re.compile(\n",
    "                    r\"\\b(weather|traffic|politics|government|election)\\b\", re.IGNORECASE\n",
    "                ),\n",
    "                re.compile(r\"\\b(my car|my phone|personal|unrelated)\\b\", re.IGNORECASE),\n",
    "            ],\n",
    "            \"Fake_Rant\": [\n",
    "                re.compile(\n",
    "                    r\"\\b(never visited|never been|never went|haven\\'t been)\\b\",\n",
    "                    re.IGNORECASE,\n",
    "                ),\n",
    "                re.compile(\n",
    "                    r\"\\b(heard bad things|rumor|heard from|people say)\\b\", re.IGNORECASE\n",
    "                ),\n",
    "                re.compile(\n",
    "                    r\"\\b(avoid|stay away|don\\'t go|waste of time)\\b\", re.IGNORECASE\n",
    "                ),\n",
    "                re.compile(\n",
    "                    r\"\\b(probably|seems like|looks like|appears)\\b.*\\b(bad|terrible|awful)\\b\",\n",
    "                    re.IGNORECASE,\n",
    "                ),\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        # Count patterns for performance metrics\n",
    "        total_patterns = sum(len(patterns) for patterns in self.patterns.values())\n",
    "        print(f\"üîç Regex Engine Initialized:\")\n",
    "        for label, patterns in self.patterns.items():\n",
    "            print(f\"   {label}: {len(patterns)} patterns\")\n",
    "        print(f\"   Total: {total_patterns} compiled regex patterns\")\n",
    "\n",
    "    def classify_text(self, text: str) -> Set[str]:\n",
    "        \"\"\"Apply regex rules to classify text - returns set of matching labels\"\"\"\n",
    "        if not text or pd.isna(text) or text.strip() == \"\":\n",
    "            return set()\n",
    "\n",
    "        labels = set()\n",
    "        text = str(text).strip()\n",
    "\n",
    "        # Special case for very short text (Irrelevant)\n",
    "        if len(text.split()) < 5:\n",
    "            labels.add(\"Irrelevant\")\n",
    "\n",
    "        # Apply regex patterns\n",
    "        for label, patterns in self.patterns.items():\n",
    "            for pattern in patterns:\n",
    "                if pattern.search(text):\n",
    "                    labels.add(label)\n",
    "                    break  # One match per category is enough\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def classify_dataframe(self, df: pd.DataFrame, text_column: str) -> pd.Series:\n",
    "        \"\"\"Lightning-fast regex classification with vectorized operations - ERROR-FREE\"\"\"\n",
    "        print(f\"üöÄ Lightning-Fast regex classification on {len(df):,} rows...\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Initialize results array - much faster than pandas operations\n",
    "        results = [set() for _ in range(len(df))]\n",
    "\n",
    "        # Get text data as numpy array for maximum speed\n",
    "        text_data = df[text_column].fillna(\"\").astype(str).values\n",
    "\n",
    "        print(f\"   ‚ö° Processing text data...\")\n",
    "\n",
    "        # Vectorized short text detection first (most common case)\n",
    "        for i, text in enumerate(text_data):\n",
    "            if text and len(str(text).split()) < 5:\n",
    "                results[i].add(\"Irrelevant\")\n",
    "\n",
    "        # Apply regex patterns efficiently\n",
    "        patterns_applied = 0\n",
    "        for label, patterns in self.patterns.items():\n",
    "            for pattern in patterns:\n",
    "                matches = 0\n",
    "                for i, text in enumerate(text_data):\n",
    "                    if text and pattern.search(str(text)):\n",
    "                        results[i].add(label)\n",
    "                        matches += 1\n",
    "\n",
    "                if matches > 0:\n",
    "                    print(f\"   üéØ {label}: {matches:,} matches\")\n",
    "                    patterns_applied += 1\n",
    "\n",
    "        # Convert results to pandas Series efficiently\n",
    "        final_results = pd.Series(results, index=df.index)\n",
    "\n",
    "        # Performance metrics\n",
    "        duration = time.time() - start_time\n",
    "        labeled_count = sum(len(labels) for labels in results)\n",
    "        rows_with_labels = sum(1 for labels in results if len(labels) > 0)\n",
    "\n",
    "        print(f\"\\n‚úÖ Lightning-Fast regex completed:\")\n",
    "        print(f\"   ‚è±Ô∏è  Time: {duration:.2f} seconds\")\n",
    "        print(f\"   ‚ö° Speed: {len(df)/duration:,.0f} rows/second\")\n",
    "        print(f\"   üéØ Patterns applied: {patterns_applied}\")\n",
    "        print(\n",
    "            f\"   üìä Rows labeled: {rows_with_labels:,} ({rows_with_labels/len(df)*100:.1f}%)\"\n",
    "        )\n",
    "\n",
    "        return final_results\n",
    "\n",
    "\n",
    "# Initialize the fast rule classifier\n",
    "rule_classifier = FastRuleClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc6fa53",
   "metadata": {},
   "source": [
    "## ü§ñ Zero-Shot Classification Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33d90163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Zero-Shot Classifier Configuration:\n",
      "   Model: facebook/bart-large-mnli\n",
      "   Score threshold: 0.8\n",
      "   Labels: ['Advertisement', 'Irrelevant', 'Fake_Rant']\n",
      "   Device: CPU\n"
     ]
    }
   ],
   "source": [
    "class ZeroShotClassifier:\n",
    "    \"\"\"Hugging Face zero-shot classification for ambiguous cases\"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"facebook/bart-large-mnli\", score_threshold=0.8):\n",
    "        self.model_name = model_name\n",
    "        self.score_threshold = score_threshold\n",
    "        self.candidate_labels = [\"Advertisement\", \"Irrelevant\", \"Fake_Rant\"]\n",
    "        self.pipeline = None\n",
    "\n",
    "        print(f\"ü§ñ Zero-Shot Classifier Configuration:\")\n",
    "        print(f\"   Model: {model_name}\")\n",
    "        print(f\"   Score threshold: {score_threshold}\")\n",
    "        print(f\"   Labels: {self.candidate_labels}\")\n",
    "        print(f\"   Device: {device_name}\")\n",
    "\n",
    "    def load_pipeline(self):\n",
    "        \"\"\"Load optimized pipeline for 20K+ rows without memory issues\"\"\"\n",
    "        print(f\"‚è≥ Loading {self.model_name} with memory optimizations...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Conservative configuration for stability with large datasets\n",
    "            self.pipeline = pipeline(\n",
    "                \"zero-shot-classification\",\n",
    "                model=self.model_name,\n",
    "                device=device,\n",
    "                torch_dtype=(\n",
    "                    torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "                ),\n",
    "                model_kwargs={\"cache_dir\": \".cache/huggingface\"},\n",
    "                return_all_scores=True,  # Get all scores for better thresholding\n",
    "            )\n",
    "\n",
    "            load_time = time.time() - start_time\n",
    "            print(f\"‚úÖ Pipeline loaded in {load_time:.1f} seconds\")\n",
    "\n",
    "            # Test with a simple example\n",
    "            test_result = self.pipeline(\"test review\", self.candidate_labels)\n",
    "            print(f\"‚ö° Pipeline ready for lightning-fast batch processing!\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load pipeline: {e}\")\n",
    "            print(f\"üí° Will use rule-based classification only\")\n",
    "            return False\n",
    "\n",
    "    def classify_text(self, text: str) -> Set[str]:\n",
    "        \"\"\"Classify text using zero-shot classification\"\"\"\n",
    "        if not self.pipeline or not text or pd.isna(text) or text.strip() == \"\":\n",
    "            return set()\n",
    "\n",
    "        try:\n",
    "            # Get predictions\n",
    "            result = self.pipeline(text, self.candidate_labels)\n",
    "\n",
    "            # Extract labels with scores above threshold\n",
    "            labels = set()\n",
    "            for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
    "                if score >= self.score_threshold:\n",
    "                    labels.add(label)\n",
    "\n",
    "            return labels\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Classification error for text: {text[:50]}... - {e}\")\n",
    "            return set()\n",
    "\n",
    "    def classify_batch(self, texts: List[str]) -> List[Set[str]]:\n",
    "        \"\"\"Lightning-fast batch processing - optimized for 20K+ rows without errors\"\"\"\n",
    "        if not self.pipeline:\n",
    "            return [set() for _ in texts]\n",
    "\n",
    "        # Optimized batch size for memory efficiency and speed\n",
    "        batch_size = 64  # Sweet spot for most GPUs\n",
    "        results = [set() for _ in texts]\n",
    "\n",
    "        # Pre-process texts to avoid errors during inference\n",
    "        processed_texts = []\n",
    "        text_indices = []\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            if text and pd.notna(text):\n",
    "                clean_text = str(text).strip()\n",
    "                if clean_text and len(clean_text) > 0:\n",
    "                    # Truncate very long texts to prevent memory issues\n",
    "                    if len(clean_text) > 512:\n",
    "                        clean_text = clean_text[:512]\n",
    "                    processed_texts.append(clean_text)\n",
    "                    text_indices.append(i)\n",
    "\n",
    "        if not processed_texts:\n",
    "            return results\n",
    "\n",
    "        print(\n",
    "            f\"‚ö° Lightning batch processing {len(processed_texts):,} texts (batch_size={batch_size})...\"\n",
    "        )\n",
    "\n",
    "        # Process in batches with error handling\n",
    "        total_batches = (len(processed_texts) + batch_size - 1) // batch_size\n",
    "\n",
    "        for batch_idx in tqdm(\n",
    "            range(0, len(processed_texts), batch_size),\n",
    "            desc=f\"‚ö° Processing {total_batches} batches\",\n",
    "            leave=False,\n",
    "        ):\n",
    "\n",
    "            batch_texts = processed_texts[batch_idx : batch_idx + batch_size]\n",
    "            batch_indices = text_indices[batch_idx : batch_idx + batch_size]\n",
    "\n",
    "            try:\n",
    "                # Single batch inference - much more reliable\n",
    "                if len(batch_texts) == 1:\n",
    "                    # Single text\n",
    "                    result = self.pipeline(batch_texts[0], self.candidate_labels)\n",
    "                    batch_results = [result]\n",
    "                else:\n",
    "                    # Multiple texts\n",
    "                    batch_results = self.pipeline(batch_texts, self.candidate_labels)\n",
    "\n",
    "                # Process results safely\n",
    "                if isinstance(batch_results, list):\n",
    "                    for j, result in enumerate(batch_results):\n",
    "                        if j < len(batch_indices):  # Safety check\n",
    "                            original_idx = batch_indices[j]\n",
    "                            labels = set()\n",
    "\n",
    "                            # Safe result processing\n",
    "                            if (\n",
    "                                isinstance(result, dict)\n",
    "                                and \"labels\" in result\n",
    "                                and \"scores\" in result\n",
    "                            ):\n",
    "                                for label, score in zip(\n",
    "                                    result[\"labels\"], result[\"scores\"]\n",
    "                                ):\n",
    "                                    if score >= self.score_threshold:\n",
    "                                        labels.add(label)\n",
    "\n",
    "                            results[original_idx] = labels\n",
    "                else:\n",
    "                    # Single result\n",
    "                    if batch_indices:\n",
    "                        original_idx = batch_indices[0]\n",
    "                        labels = set()\n",
    "\n",
    "                        if (\n",
    "                            isinstance(batch_results, dict)\n",
    "                            and \"labels\" in batch_results\n",
    "                            and \"scores\" in batch_results\n",
    "                        ):\n",
    "                            for label, score in zip(\n",
    "                                batch_results[\"labels\"], batch_results[\"scores\"]\n",
    "                            ):\n",
    "                                if score >= self.score_threshold:\n",
    "                                    labels.add(label)\n",
    "\n",
    "                        results[original_idx] = labels\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"‚ö†Ô∏è  Batch {batch_idx//batch_size + 1}/{total_batches} failed: {e}\"\n",
    "                )\n",
    "                # Continue with empty results for this batch\n",
    "                continue\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialize zero-shot classifier\n",
    "zero_shot_classifier = ZeroShotClassifier(score_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6e75923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading facebook/bart-large-mnli with memory optimizations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline loaded in 2.7 seconds\n",
      "‚ö° Pipeline ready for lightning-fast batch processing!\n",
      "\n",
      "üéØ Ready for hybrid classification:\n",
      "   1. Regex rules for obvious patterns (ultra-fast)\n",
      "   2. Zero-shot classification for ambiguous cases (accurate)\n",
      "‚ö° Pipeline ready for lightning-fast batch processing!\n",
      "\n",
      "üéØ Ready for hybrid classification:\n",
      "   1. Regex rules for obvious patterns (ultra-fast)\n",
      "   2. Zero-shot classification for ambiguous cases (accurate)\n"
     ]
    }
   ],
   "source": [
    "# Load the zero-shot pipeline (this may take a moment for first-time download)\n",
    "pipeline_loaded = zero_shot_classifier.load_pipeline()\n",
    "\n",
    "if pipeline_loaded:\n",
    "    print(\"\\nüéØ Ready for hybrid classification:\")\n",
    "    print(\"   1. Regex rules for obvious patterns (ultra-fast)\")\n",
    "    print(\"   2. Zero-shot classification for ambiguous cases (accurate)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Zero-shot classification unavailable - using regex rules only\")\n",
    "    print(\"   This may happen due to memory constraints or model loading issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ebafd9",
   "metadata": {},
   "source": [
    "## ‚ö° Hybrid Classification Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b38ad809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Hybrid Classifier ready for lightning-fast classification!\n"
     ]
    }
   ],
   "source": [
    "class HybridClassifier:\n",
    "    \"\"\"Combines regex rules and zero-shot classification for optimal performance\"\"\"\n",
    "\n",
    "    def __init__(self, rule_classifier, zero_shot_classifier):\n",
    "        self.rule_classifier = rule_classifier\n",
    "        self.zero_shot_classifier = zero_shot_classifier\n",
    "\n",
    "    def classify_dataframe(self, df: pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
    "        \"\"\"Apply hybrid classification to entire dataframe with MAXIMUM SPEED\"\"\"\n",
    "        print(f\"üî• Starting ULTRA-FAST Hybrid Classification Pipeline\")\n",
    "        print(f\"üìä Processing {len(df):,} rows\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Step 1: Apply regex rules to all rows (already optimized)\n",
    "        print(f\"\\nüîç Step 1: Applying regex rules...\")\n",
    "        rule_labels = self.rule_classifier.classify_dataframe(df, text_column)\n",
    "\n",
    "        # Step 2: Identify rows that need zero-shot classification\n",
    "        unlabeled_mask = rule_labels.apply(len) == 0\n",
    "        unlabeled_count = unlabeled_mask.sum()\n",
    "\n",
    "        print(f\"\\nüìã Classification Status:\")\n",
    "        print(f\"   Rule-based labels: {(~unlabeled_mask).sum():,} rows\")\n",
    "        print(f\"   Needs zero-shot: {unlabeled_count:,} rows\")\n",
    "\n",
    "        # Step 3: Apply zero-shot classification to unlabeled rows\n",
    "        final_labels = rule_labels.copy()\n",
    "\n",
    "        if unlabeled_count > 0 and self.zero_shot_classifier.pipeline:\n",
    "            print(\n",
    "                f\"\\nüöÄ Step 2: ULTRA-FAST zero-shot classification on {unlabeled_count:,} rows...\"\n",
    "            )\n",
    "\n",
    "            # Extract unlabeled texts more efficiently\n",
    "            unlabeled_texts = df.loc[unlabeled_mask, text_column].values.tolist()\n",
    "            zero_shot_start = time.time()\n",
    "\n",
    "            # Use optimized batch processing\n",
    "            zero_shot_labels = self.zero_shot_classifier.classify_batch(unlabeled_texts)\n",
    "\n",
    "            zero_shot_duration = time.time() - zero_shot_start\n",
    "            print(f\"   ‚è±Ô∏è  Zero-shot time: {zero_shot_duration:.2f} seconds\")\n",
    "            if zero_shot_duration > 0:\n",
    "                print(\n",
    "                    f\"   ‚ö° Zero-shot speed: {unlabeled_count/zero_shot_duration:,.0f} rows/second\"\n",
    "                )\n",
    "\n",
    "            # Merge zero-shot results with rule-based results using vectorized operations\n",
    "            unlabeled_indices = df.index[unlabeled_mask]\n",
    "            for idx, labels in zip(unlabeled_indices, zero_shot_labels):\n",
    "                final_labels.loc[idx] = labels\n",
    "\n",
    "        elif unlabeled_count > 0:\n",
    "            print(\n",
    "                f\"\\n‚ö†Ô∏è  Zero-shot classifier not available - {unlabeled_count:,} rows remain unlabeled\"\n",
    "            )\n",
    "\n",
    "        # Step 4: Create final results with vectorized operations\n",
    "        df_result = df.copy()\n",
    "\n",
    "        # Convert sets to lists efficiently\n",
    "        labels_list = final_labels.apply(list)\n",
    "        df_result[\"labels\"] = labels_list\n",
    "\n",
    "        # Vectorized boolean column creation for maximum speed\n",
    "        print(f\"\\n‚ö° Creating boolean columns with vectorized operations...\")\n",
    "\n",
    "        # Create boolean arrays directly\n",
    "        advertisement_mask = final_labels.apply(lambda x: \"Advertisement\" in x).values\n",
    "        irrelevant_mask = final_labels.apply(lambda x: \"Irrelevant\" in x).values\n",
    "        fake_rant_mask = final_labels.apply(lambda x: \"Fake_Rant\" in x).values\n",
    "\n",
    "        df_result[\"is_advertisement\"] = advertisement_mask\n",
    "        df_result[\"is_irrelevant\"] = irrelevant_mask\n",
    "        df_result[\"is_fake_rant\"] = fake_rant_mask\n",
    "\n",
    "        # Calculate final statistics\n",
    "        total_duration = time.time() - start_time\n",
    "        labeled_rows = (final_labels.apply(len) > 0).sum()\n",
    "        total_labels = final_labels.apply(len).sum()\n",
    "\n",
    "        print(f\"\\nüéâ ULTRA-FAST Hybrid Classification Complete!\")\n",
    "        print(f\"   ‚è±Ô∏è  Total time: {total_duration:.2f} seconds\")\n",
    "        print(f\"   ‚ö° BLAZING speed: {len(df)/total_duration:,.0f} rows/second\")\n",
    "        print(f\"   üéØ Labeled rows: {labeled_rows:,} ({labeled_rows/len(df)*100:.1f}%)\")\n",
    "        print(f\"   üè∑Ô∏è  Total labels: {total_labels}\")\n",
    "\n",
    "        # Label distribution with efficient counting\n",
    "        ad_count = advertisement_mask.sum()\n",
    "        irrelevant_count = irrelevant_mask.sum()\n",
    "        fake_rant_count = fake_rant_mask.sum()\n",
    "        clean_count = (\n",
    "            len(df_result)\n",
    "            - (advertisement_mask | irrelevant_mask | fake_rant_mask).sum()\n",
    "        )\n",
    "\n",
    "        print(f\"\\nüìä Label Distribution:\")\n",
    "        print(f\"   Advertisement: {ad_count:,} rows\")\n",
    "        print(f\"   Irrelevant: {irrelevant_count:,} rows\")\n",
    "        print(f\"   Fake_Rant: {fake_rant_count:,} rows\")\n",
    "        print(f\"   Clean (no labels): {clean_count:,} rows\")\n",
    "\n",
    "        return df_result\n",
    "\n",
    "\n",
    "# Initialize hybrid classifier\n",
    "hybrid_classifier = HybridClassifier(rule_classifier, zero_shot_classifier)\n",
    "print(\"‚ö° Hybrid Classifier ready for lightning-fast classification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37b1eae",
   "metadata": {},
   "source": [
    "## üöÄ Execute Classification Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7345da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>review_time</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>pics</th>\n",
       "      <th>resp</th>\n",
       "      <th>gmap_id</th>\n",
       "      <th>has_resp</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>resp_time</th>\n",
       "      <th>biz_name</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>num_of_reviews</th>\n",
       "      <th>price_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103563353519118155776</td>\n",
       "      <td>Peri Gray</td>\n",
       "      <td>2018-01-16 17:11:15.780000+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Great place to care for our children.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x532af45db8f30779:0xd9be9359f1e56178</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CRST WIC Office</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101824980797027237888</td>\n",
       "      <td>Suzy Berndt</td>\n",
       "      <td>2018-07-30 03:45:50.314000+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Th sw y are so nice</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x532af45db8f30779:0xd9be9359f1e56178</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CRST WIC Office</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108711640480272777216</td>\n",
       "      <td>Rosemary Red Legs</td>\n",
       "      <td>2018-07-07 13:11:33.932000+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Went with my daughter</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x532af45db8f30779:0xd9be9359f1e56178</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CRST WIC Office</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101852294221648461824</td>\n",
       "      <td>Brown Wolf</td>\n",
       "      <td>2018-09-16 08:13:55.922000+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x532af45db8f30779:0xd9be9359f1e56178</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CRST WIC Office</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>108987444312280645632</td>\n",
       "      <td>C J Blue Coat</td>\n",
       "      <td>2016-09-26 20:39:35.491000+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x532af45db8f30779:0xd9be9359f1e56178</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CRST WIC Office</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user_id          user_name                       review_time  \\\n",
       "0  103563353519118155776          Peri Gray  2018-01-16 17:11:15.780000+00:00   \n",
       "1  101824980797027237888        Suzy Berndt  2018-07-30 03:45:50.314000+00:00   \n",
       "2  108711640480272777216  Rosemary Red Legs  2018-07-07 13:11:33.932000+00:00   \n",
       "3  101852294221648461824         Brown Wolf  2018-09-16 08:13:55.922000+00:00   \n",
       "4  108987444312280645632      C J Blue Coat  2016-09-26 20:39:35.491000+00:00   \n",
       "\n",
       "  rating                            review_text   pics resp  \\\n",
       "0      5  Great place to care for our children.  False  NaN   \n",
       "1      5                    Th sw y are so nice  False  NaN   \n",
       "2      5                  Went with my daughter  False  NaN   \n",
       "3      2                                         False  NaN   \n",
       "4      5                                         False  NaN   \n",
       "\n",
       "                                 gmap_id has_resp resp_text resp_time  \\\n",
       "0  0x532af45db8f30779:0xd9be9359f1e56178    False       NaN       NaN   \n",
       "1  0x532af45db8f30779:0xd9be9359f1e56178    False       NaN       NaN   \n",
       "2  0x532af45db8f30779:0xd9be9359f1e56178    False       NaN       NaN   \n",
       "3  0x532af45db8f30779:0xd9be9359f1e56178    False       NaN       NaN   \n",
       "4  0x532af45db8f30779:0xd9be9359f1e56178    False       NaN       NaN   \n",
       "\n",
       "          biz_name description category  avg_rating  num_of_reviews  \\\n",
       "0  CRST WIC Office         NaN      NaN         4.7             8.0   \n",
       "1  CRST WIC Office         NaN      NaN         4.7             8.0   \n",
       "2  CRST WIC Office         NaN      NaN         4.7             8.0   \n",
       "3  CRST WIC Office         NaN      NaN         4.7             8.0   \n",
       "4  CRST WIC Office         NaN      NaN         4.7             8.0   \n",
       "\n",
       "   price_level  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          0.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ac7724a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Executing Complete Multi-Label Classification Pipeline\n",
      "============================================================\n",
      "Sampling data from Main Dataframe...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>review_time</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>pics</th>\n",
       "      <th>resp</th>\n",
       "      <th>gmap_id</th>\n",
       "      <th>has_resp</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>resp_time</th>\n",
       "      <th>biz_name</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>num_of_reviews</th>\n",
       "      <th>price_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103563353519118155776</td>\n",
       "      <td>Peri Gray</td>\n",
       "      <td>2018-01-16 17:11:15.780000+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Great place to care for our children.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x532af45db8f30779:0xd9be9359f1e56178</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CRST WIC Office</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101824980797027237888</td>\n",
       "      <td>Suzy Berndt</td>\n",
       "      <td>2018-07-30 03:45:50.314000+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Th sw y are so nice</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x532af45db8f30779:0xd9be9359f1e56178</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CRST WIC Office</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108711640480272777216</td>\n",
       "      <td>Rosemary Red Legs</td>\n",
       "      <td>2018-07-07 13:11:33.932000+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Went with my daughter</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x532af45db8f30779:0xd9be9359f1e56178</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CRST WIC Office</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user_id          user_name                       review_time  \\\n",
       "0  103563353519118155776          Peri Gray  2018-01-16 17:11:15.780000+00:00   \n",
       "1  101824980797027237888        Suzy Berndt  2018-07-30 03:45:50.314000+00:00   \n",
       "2  108711640480272777216  Rosemary Red Legs  2018-07-07 13:11:33.932000+00:00   \n",
       "\n",
       "  rating                            review_text   pics resp  \\\n",
       "0      5  Great place to care for our children.  False  NaN   \n",
       "1      5                    Th sw y are so nice  False  NaN   \n",
       "2      5                  Went with my daughter  False  NaN   \n",
       "\n",
       "                                 gmap_id has_resp resp_text resp_time  \\\n",
       "0  0x532af45db8f30779:0xd9be9359f1e56178    False       NaN       NaN   \n",
       "1  0x532af45db8f30779:0xd9be9359f1e56178    False       NaN       NaN   \n",
       "2  0x532af45db8f30779:0xd9be9359f1e56178    False       NaN       NaN   \n",
       "\n",
       "          biz_name description category  avg_rating  num_of_reviews  \\\n",
       "0  CRST WIC Office         NaN      NaN         4.7             8.0   \n",
       "1  CRST WIC Office         NaN      NaN         4.7             8.0   \n",
       "2  CRST WIC Office         NaN      NaN         4.7             8.0   \n",
       "\n",
       "   price_level  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the complete hybrid classification pipeline\n",
    "print(\"üöÄ Executing Complete Multi-Label Classification Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sampling data from Main Dataframe...\")\n",
    "\n",
    "\n",
    "def rating_diverse_sample(df, sample_size=10_000):\n",
    "    # Sample proportionally by rating\n",
    "    rating_counts = df[\"rating\"].value_counts()\n",
    "    diverse_sample = (\n",
    "        df.groupby(\"rating\", group_keys=False)\n",
    "        .apply(\n",
    "            lambda x: x.sample(\n",
    "                min(len(x), int(sample_size * len(x) / len(df))), random_state=42\n",
    "            )\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return diverse_sample\n",
    "\n",
    "\n",
    "sampled_df = rating_diverse_sample(df)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c482e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Starting ULTRA-FAST Hybrid Classification Pipeline\n",
      "üìä Processing 9,996 rows\n",
      "\n",
      "üîç Step 1: Applying regex rules...\n",
      "üöÄ Lightning-Fast regex classification on 9,996 rows...\n",
      "   ‚ö° Processing text data...\n",
      "   üéØ Advertisement: 88 matches\n",
      "   üéØ Advertisement: 95 matches\n",
      "   üéØ Advertisement: 47 matches\n",
      "   üéØ Advertisement: 57 matches\n",
      "   üéØ Irrelevant: 9 matches\n",
      "   üéØ Irrelevant: 56 matches\n",
      "   üéØ Irrelevant: 11 matches\n",
      "   üéØ Irrelevant: 27 matches\n",
      "   üéØ Fake_Rant: 12 matches\n",
      "   üéØ Fake_Rant: 11 matches\n",
      "   üéØ Fake_Rant: 1 matches\n",
      "\n",
      "‚úÖ Lightning-Fast regex completed:\n",
      "   ‚è±Ô∏è  Time: 0.12 seconds\n",
      "   ‚ö° Speed: 81,311 rows/second\n",
      "   üéØ Patterns applied: 11\n",
      "   üìä Rows labeled: 1,618 (16.2%)\n",
      "\n",
      "üìã Classification Status:\n",
      "   Rule-based labels: 1,618 rows\n",
      "   Needs zero-shot: 8,378 rows\n",
      "\n",
      "üöÄ Step 2: ULTRA-FAST zero-shot classification on 8,378 rows...\n",
      "‚ö° Lightning batch processing 3,535 texts (batch_size=64)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚è±Ô∏è  Zero-shot time: 1091.81 seconds\n",
      "   ‚ö° Zero-shot speed: 8 rows/second\n",
      "\n",
      "‚ö° Creating boolean columns with vectorized operations...\n",
      "\n",
      "üéâ ULTRA-FAST Hybrid Classification Complete!\n",
      "   ‚è±Ô∏è  Total time: 1092.07 seconds\n",
      "   ‚ö° BLAZING speed: 9 rows/second\n",
      "   üéØ Labeled rows: 2,663 (26.6%)\n",
      "   üè∑Ô∏è  Total labels: 2689\n",
      "\n",
      "üìä Label Distribution:\n",
      "   Advertisement: 1,307 rows\n",
      "   Irrelevant: 1,357 rows\n",
      "   Fake_Rant: 25 rows\n",
      "   Clean (no labels): 7,333 rows\n",
      "\n",
      "‚úÖ Classification pipeline completed successfully!\n",
      "üìä Results saved in 'labels' column as list of strings\n",
      "ÔøΩ Individual boolean columns added for easy filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Apply hybrid classification\n",
    "df_classified = hybrid_classifier.classify_dataframe(sampled_df, text_col)\n",
    "\n",
    "print(f\"\\n‚úÖ Classification pipeline completed successfully!\")\n",
    "print(f\"üìä Results saved in 'labels' column as list of strings\")\n",
    "print(f\"ÔøΩ Individual boolean columns added for easy filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c58bb5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VALIDATION PASSED - df_classified exists!\n",
      "üìä Quick Stats:\n",
      "   Rows: 9,996\n",
      "   Columns: 21\n",
      "   ‚úÖ All required columns present: ['labels', 'is_advertisement', 'is_irrelevant', 'is_fake_rant']\n",
      "\n",
      "üè∑Ô∏è Label Distribution:\n",
      "   Advertisement: 1,307 (13.1%)\n",
      "   Irrelevant: 1,357 (13.6%)\n",
      "   Fake_Rant: 25 (0.3%)\n",
      "   Clean: 7,307 (73.1%)\n",
      "\n",
      "üéØ SUCCESS! Ready for detailed analysis!\n"
     ]
    }
   ],
   "source": [
    "# üîç Quick Results Validation\n",
    "if \"df_classified\" in locals():\n",
    "    print(\"‚úÖ VALIDATION PASSED - df_classified exists!\")\n",
    "    print(f\"üìä Quick Stats:\")\n",
    "    print(f\"   Rows: {len(df_classified):,}\")\n",
    "    print(f\"   Columns: {len(df_classified.columns)}\")\n",
    "\n",
    "    # Check for required columns\n",
    "    required_cols = [\"labels\", \"is_advertisement\", \"is_irrelevant\", \"is_fake_rant\"]\n",
    "    missing_cols = [col for col in required_cols if col not in df_classified.columns]\n",
    "\n",
    "    if not missing_cols:\n",
    "        print(f\"   ‚úÖ All required columns present: {required_cols}\")\n",
    "\n",
    "        # Quick label distribution\n",
    "        label_counts = {\n",
    "            \"Advertisement\": df_classified[\"is_advertisement\"].sum(),\n",
    "            \"Irrelevant\": df_classified[\"is_irrelevant\"].sum(),\n",
    "            \"Fake_Rant\": df_classified[\"is_fake_rant\"].sum(),\n",
    "        }\n",
    "\n",
    "        print(f\"\\nüè∑Ô∏è Label Distribution:\")\n",
    "        for label, count in label_counts.items():\n",
    "            pct = (count / len(df_classified)) * 100\n",
    "            print(f\"   {label}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "        clean_count = len(df_classified) - sum(label_counts.values())\n",
    "        clean_pct = (clean_count / len(df_classified)) * 100\n",
    "        print(f\"   Clean: {clean_count:,} ({clean_pct:.1f}%)\")\n",
    "\n",
    "        print(f\"\\nüéØ SUCCESS! Ready for detailed analysis!\")\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ùå Missing columns: {missing_cols}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå VALIDATION FAILED - df_classified not found\")\n",
    "    print(\"üí° Run the classification cell above first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6fa238",
   "metadata": {},
   "source": [
    "## üìã Results Analysis and Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ba8fc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ CLASSIFICATION RESULTS ANALYSIS\n",
      "==================================================\n",
      "\n",
      "üìä Overall Statistics:\n",
      "   Total rows: 9,996\n",
      "   Flagged rows: 2,663 (26.6%)\n",
      "   Clean rows: 7,333 (73.4%)\n",
      "\n",
      "üè∑Ô∏è  Label Breakdown:\n",
      "   üì¢ Advertisement: 1,307 (13.1%)\n",
      "   üö´ Irrelevant: 1,357 (13.6%)\n",
      "   üò° Fake_Rant: 25 (0.3%)\n",
      "   üîÑ Multi-label: 26 (0.3%)\n",
      "\n",
      "üìù EXAMPLE CLASSIFICATIONS:\n",
      "========================================\n",
      "\n",
      "üì¢ Advertisement Examples:\n",
      "   1. Text: 'Took 8 hours to set up my account in store with these guys. Was billed for stuff I never bought. For 3 months I've calle...'\n",
      "      Labels: [Fake_Rant, Advertisement]\n",
      "      Rating: 1\n",
      "\n",
      "   2. Text: 'Biggest RIP OFF ever! For a family of 5 to see a movie, one large popcorn and 1 fountain drink and 4 waters it was $100....'\n",
      "      Labels: [Advertisement]\n",
      "      Rating: 1\n",
      "\n",
      "   3. Text: 'They charge money for air'\n",
      "      Labels: [Advertisement]\n",
      "      Rating: 1\n",
      "\n",
      "\n",
      "üö´ Irrelevant Examples:\n",
      "   1. Text: 'Didn't take my name'\n",
      "      Labels: [Irrelevant]\n",
      "      Rating: 1\n",
      "\n",
      "   2. Text: 'Wal Mart.'\n",
      "      Labels: [Irrelevant]\n",
      "      Rating: 1\n",
      "\n",
      "   3. Text: 'No healthy options'\n",
      "      Labels: [Irrelevant]\n",
      "      Rating: 1\n",
      "\n",
      "\n",
      "üò° Fake_Rant Examples:\n",
      "   1. Text: 'Very unfriendly and unhelpful staff, we asked to have our numbers ported to Verizon and they looked at our connections a...'\n",
      "      Labels: [Fake_Rant]\n",
      "      Rating: 1\n",
      "\n",
      "   2. Text: 'Took 8 hours to set up my account in store with these guys. Was billed for stuff I never bought. For 3 months I've calle...'\n",
      "      Labels: [Fake_Rant, Advertisement]\n",
      "      Rating: 1\n",
      "\n",
      "   3. Text: 'I am beyond-confused at the positive reviews here.  Bartender was unfriendly; it appeared we were interrupting her life ...'\n",
      "      Labels: [Fake_Rant]\n",
      "      Rating: 1\n",
      "\n",
      "\n",
      "‚úÖ Clean Examples:\n",
      "   1. Text: 'They are screwing me over on my disability just cause im working'\n",
      "      Labels: [None]\n",
      "      Rating: 1\n",
      "\n",
      "   2. Text: 'The worst experience of my life!  The food was okay mine wasn't hot just warm and my daughter's was way too sweet!  The ...'\n",
      "      Labels: [None]\n",
      "      Rating: 1\n",
      "\n",
      "   3. Text: ''\n",
      "      Labels: [None]\n",
      "      Rating: 1\n",
      "\n",
      "\n",
      "üíæ Final Dataset:\n",
      "   Shape: (9996, 21)\n",
      "   New columns: labels, is_advertisement, is_irrelevant, is_fake_rant\n",
      "   Memory usage: 10.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Display detailed results with examples\n",
    "print(\"üéØ CLASSIFICATION RESULTS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Overall statistics\n",
    "total_rows = len(df_classified)\n",
    "labeled_rows = (df_classified[\"labels\"].apply(len) > 0).sum()\n",
    "clean_rows = total_rows - labeled_rows\n",
    "\n",
    "print(f\"\\nüìä Overall Statistics:\")\n",
    "print(f\"   Total rows: {total_rows:,}\")\n",
    "print(f\"   Flagged rows: {labeled_rows:,} ({labeled_rows/total_rows*100:.1f}%)\")\n",
    "print(f\"   Clean rows: {clean_rows:,} ({clean_rows/total_rows*100:.1f}%)\")\n",
    "\n",
    "# Label-specific statistics\n",
    "print(f\"\\nüè∑Ô∏è  Label Breakdown:\")\n",
    "ad_count = df_classified[\"is_advertisement\"].sum()\n",
    "irrelevant_count = df_classified[\"is_irrelevant\"].sum()\n",
    "fake_rant_count = df_classified[\"is_fake_rant\"].sum()\n",
    "\n",
    "print(f\"   üì¢ Advertisement: {ad_count:,} ({ad_count/total_rows*100:.1f}%)\")\n",
    "print(\n",
    "    f\"   üö´ Irrelevant: {irrelevant_count:,} ({irrelevant_count/total_rows*100:.1f}%)\"\n",
    ")\n",
    "print(f\"   üò° Fake_Rant: {fake_rant_count:,} ({fake_rant_count/total_rows*100:.1f}%)\")\n",
    "\n",
    "# Multi-label statistics\n",
    "multi_label_mask = df_classified[\"labels\"].apply(len) > 1\n",
    "multi_label_count = multi_label_mask.sum()\n",
    "print(\n",
    "    f\"   üîÑ Multi-label: {multi_label_count:,} ({multi_label_count/total_rows*100:.1f}%)\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù EXAMPLE CLASSIFICATIONS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Show examples for each label category\n",
    "categories = {\n",
    "    \"üì¢ Advertisement Examples\": df_classified[df_classified[\"is_advertisement\"]],\n",
    "    \"üö´ Irrelevant Examples\": df_classified[df_classified[\"is_irrelevant\"]],\n",
    "    \"üò° Fake_Rant Examples\": df_classified[df_classified[\"is_fake_rant\"]],\n",
    "    \"‚úÖ Clean Examples\": df_classified[\n",
    "        ~(\n",
    "            df_classified[\"is_advertisement\"]\n",
    "            | df_classified[\"is_irrelevant\"]\n",
    "            | df_classified[\"is_fake_rant\"]\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "\n",
    "for category_name, category_df in categories.items():\n",
    "    if len(category_df) > 0:\n",
    "        print(f\"\\n{category_name}:\")\n",
    "        sample_size = min(3, len(category_df))\n",
    "\n",
    "        for i, (idx, row) in enumerate(category_df.head(sample_size).iterrows()):\n",
    "            text_preview = (\n",
    "                row[text_col][:120] + \"...\"\n",
    "                if len(row[text_col]) > 120\n",
    "                else row[text_col]\n",
    "            )\n",
    "            labels_str = \", \".join(row[\"labels\"]) if row[\"labels\"] else \"None\"\n",
    "\n",
    "            print(f\"   {i+1}. Text: '{text_preview}'\")\n",
    "            print(f\"      Labels: [{labels_str}]\")\n",
    "            if \"rating\" in row:\n",
    "                print(f\"      Rating: {row['rating']}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"\\n{category_name}: No examples found\")\n",
    "\n",
    "# Display final dataset info\n",
    "print(f\"\\nüíæ Final Dataset:\")\n",
    "print(f\"   Shape: {df_classified.shape}\")\n",
    "print(f\"   New columns: labels, is_advertisement, is_irrelevant, is_fake_rant\")\n",
    "print(\n",
    "    f\"   Memory usage: {df_classified.memory_usage(deep=True).sum() / 1024**2:.1f} MB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7822ed58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Took 8 hours to set up my account in store wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Biggest RIP OFF ever! For a family of 5 to see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>They charge money for air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Worst customer service I've ever experienced m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Spent $27 on 2 quesadilla meals and a super na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>Was a nice day and time at this time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>Best fast food there is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>Good beer, free pretzels and well lit overflow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>Friendly Service And Great Delicious Food üòäüòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Always a great place. Simple but well prepared...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1307 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            review_text\n",
       "13    Took 8 hours to set up my account in store wit...\n",
       "30    Biggest RIP OFF ever! For a family of 5 to see...\n",
       "54                            They charge money for air\n",
       "57    Worst customer service I've ever experienced m...\n",
       "60    Spent $27 on 2 quesadilla meals and a super na...\n",
       "...                                                 ...\n",
       "9980               Was a nice day and time at this time\n",
       "9985                            Best fast food there is\n",
       "9989  Good beer, free pretzels and well lit overflow...\n",
       "9994       Friendly Service And Great Delicious Food üòäüòä\n",
       "9995  Always a great place. Simple but well prepared...\n",
       "\n",
       "[1307 rows x 1 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classified[df_classified[\"is_advertisement\"] == True][[\"review_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260e3a3",
   "metadata": {},
   "source": [
    "## üíæ Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the classified dataset\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"../outputs/classified\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate timestamped filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_filename = f\"classified_reviews_{timestamp}.csv\"\n",
    "output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "# Save the results\n",
    "df_classified.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"üíæ Classified dataset saved:\")\n",
    "print(f\"   üìÅ Path: {output_path}\")\n",
    "print(f\"   üìä Rows: {len(df_classified):,}\")\n",
    "print(f\"   üè∑Ô∏è  Columns: {len(df_classified.columns)}\")\n",
    "\n",
    "# Save classification summary\n",
    "summary = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"total_rows\": len(df_classified),\n",
    "    \"classification_method\": \"hybrid_regex_zeroshot\",\n",
    "    \"zero_shot_model\": (\n",
    "        zero_shot_classifier.model_name if zero_shot_classifier.pipeline else \"Not used\"\n",
    "    ),\n",
    "    \"label_counts\": {\n",
    "        \"advertisement\": int(df_classified[\"is_advertisement\"].sum()),\n",
    "        \"irrelevant\": int(df_classified[\"is_irrelevant\"].sum()),\n",
    "        \"fake_rant\": int(df_classified[\"is_fake_rant\"].sum()),\n",
    "        \"clean\": int(\n",
    "            (\n",
    "                ~(\n",
    "                    df_classified[\"is_advertisement\"]\n",
    "                    | df_classified[\"is_irrelevant\"]\n",
    "                    | df_classified[\"is_fake_rant\"]\n",
    "                )\n",
    "            ).sum()\n",
    "        ),\n",
    "    },\n",
    "    \"multi_label_rows\": int((df_classified[\"labels\"].apply(len) > 1).sum()),\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(output_dir, f\"classification_summary_{timestamp}.json\")\n",
    "import json\n",
    "\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüìä Classification summary saved: {summary_path}\")\n",
    "\n",
    "print(f\"\\nüéâ MULTI-LABEL CLASSIFICATION COMPLETE!\")\n",
    "print(f\"\\n‚ú® Your data is now classified with lightning speed and high accuracy!\")\n",
    "print(f\"üî• Ready for downstream ML tasks, filtering, or analysis!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
